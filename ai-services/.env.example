# Dating App AI Services Configuration
# Copy this file to .env and adjust values as needed

# ============== Ollama Settings ==============
# Ollama server URL (use host.docker.internal in Docker)
AI_OLLAMA_HOST=http://localhost:11434

# Model to use - optimized options for GTX 1050Ti:
# - llama3.2:1b    (fastest, ~1.5GB VRAM)
# - llama3.2:3b    (balanced, ~2.5GB VRAM) [RECOMMENDED]
# - mistral:7b-instruct-q4_0  (quality, ~4GB VRAM)
AI_OLLAMA_MODEL=llama3.2:3b

# Request timeout in seconds
AI_OLLAMA_TIMEOUT=120

# ============== Generation Parameters ==============
# Lower values = faster, less creative
# Higher values = slower, more creative
AI_DEFAULT_TEMPERATURE=0.7
AI_DEFAULT_MAX_TOKENS=256
AI_DEFAULT_TOP_P=0.9
AI_DEFAULT_TOP_K=40

# ============== Rate Limiting ==============
AI_RATE_LIMIT_PER_MINUTE=30
AI_RATE_LIMIT_BURST=10

# ============== Caching ==============
AI_CACHE_TTL_SECONDS=300
AI_CACHE_MAX_SIZE=100

# ============== Server Settings ==============
AI_HOST=0.0.0.0
AI_PORT=8001
AI_WORKERS=1
AI_DEBUG=false

# ============== CORS ==============
AI_CORS_ORIGINS=http://localhost:3000,http://localhost:8000
